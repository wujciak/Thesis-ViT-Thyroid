{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "1zBD2ipDlKfx2YCpEjEBgU7W806NeIPhJ",
      "authorship_tag": "ABX9TyNC88mDG2nF3jVFSzEFJClz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wujciak/thesis_colab/blob/main/transformers_thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architektura Vision Transformer\n",
        "Zaproponowana przez Google Research Team architektura Vision Transformer  uproszczeniem klasy VisionTransformer.\n"
      ],
      "metadata": {
        "id": "5Kfe3cnsXtYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "DNFe5qiG7hZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"Split image into patches and then embed them.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  img_size : int\n",
        "    Size of the image (it is a square).\n",
        "  patch_size : int\n",
        "    Size of the patch (it is a square).\n",
        "  in_chans : int\n",
        "    Number of input channels.\n",
        "  embed_dim : int\n",
        "    The embedding dimension.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  n_patches : int\n",
        "    Number of patches inside of our image.\n",
        "  proj : nn.Conv2d\n",
        "    Convolutional layer that does both the splitting into patches\n",
        "    and their embedding.\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
        "    super().__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "    self.proj = nn.Conv2d(\n",
        "        in_chans,\n",
        "        embed_dim,\n",
        "        kernel_size=patch_size,\n",
        "        stride=patch_size,\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_chans, img_size, img_size)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches, embed_dim)'.\n",
        "    \"\"\"\n",
        "    x = self.proj(\n",
        "        x\n",
        "    )  # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
        "    x = x.flatten(2)  # (n_samples, embed_dim, n_patches)\n",
        "    x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  \"\"\"Multi-head attention mechanism.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dim : int\n",
        "    The input and out dimension of per token features.\n",
        "  n_heads : int\n",
        "    Number of attention heads.\n",
        "  qkv_bias : bool\n",
        "    If True then we include bias to the query, key and value projections.\n",
        "  attn_p : float\n",
        "    Dropout probability applied to the query, key and value tensors.\n",
        "  proj_p : float\n",
        "    Dropout probability applied to the output tensor.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  scale : float\n",
        "    Normalizing consant for the dot product.\n",
        "  qkv : nn.Linear\n",
        "    Linear projection for the query, key and value.\n",
        "  proj : nn.Linear\n",
        "    Linear mapping that takes in the concatenated output of all attention\n",
        "    heads and maps it into a new space.\n",
        "  attn_drop, proj_drop : nn.Dropout\n",
        "    Dropout layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.dim = dim\n",
        "    self.head_dim = dim // n_heads\n",
        "    self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "    \"\"\"\n",
        "    n_samples, n_tokens, dim = x.shape\n",
        "\n",
        "    if dim != self.dim:\n",
        "      raise ValueError\n",
        "\n",
        "    qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
        "    qkv = qkv.reshape(\n",
        "        n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
        "    )  # (n_samples, n_patches + 1, 3, n_heads, head_dim)\n",
        "    qkv = qkv.permute(\n",
        "        2, 0, 3, 1, 4\n",
        "    )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
        "\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "    k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
        "    dp = (\n",
        "        q @ k_t\n",
        "    ) * self.scale  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "    attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    weighted_avg = attn @ v  # (n_samples, n_heads, n_patches + 1, head_dim)\n",
        "    weighted_avg = weighted_avg.transpose(\n",
        "        1, 2\n",
        "    )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
        "    weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "    x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
        "    x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  \"\"\"Multi-layer perceptron.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  in_features : int\n",
        "    Number of input features\n",
        "  hidden_features : int\n",
        "    Number of nodes in the hidden layer\n",
        "  out_features : int\n",
        "    Number of output features\n",
        "  p : float\n",
        "    Dropout probability\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  fc : nn.Linear\n",
        "    The first linear layer\n",
        "  act : nn.GELU\n",
        "    GELU activation function\n",
        "  fc2 : nn.Linear\n",
        "    The second linear layer\n",
        "  drop : nn.Dropout\n",
        "    Dropout layer\n",
        "  \"\"\"\n",
        "  def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = nn.GELU()\n",
        "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "    self.drop = nn.Dropout(p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_features)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, out_features)'.\n",
        "    \"\"\"\n",
        "    x = self.fc1(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer block.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dim : int\n",
        "    Embedding dimension\n",
        "  n_heads : int\n",
        "    Number of attention heads\n",
        "  mlp_ratio : float\n",
        "    Determines the hidden dimension size of the `MLP` module with respect\n",
        "    to `dim`\n",
        "  qkv_bias : bool\n",
        "    If True we include bias to the query, key and value projections\n",
        "  p, attn_p : float\n",
        "    Dropout probability\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  norm1, norm2 : nn.LayerNorm\n",
        "    Layer normalization\n",
        "  attn : Attention\n",
        "    Attention module\n",
        "  mlp : MLP\n",
        "    MLP module.\n",
        "  \"\"\"\n",
        "  def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    self.attn = Attention(\n",
        "        dim,\n",
        "        n_heads=n_heads,\n",
        "        qkv_bias=qkv_bias,\n",
        "        attn_p=attn_p,\n",
        "        proj_p=p\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    hidden_features = int(dim * mlp_ratio)\n",
        "    self.mlp = MLP(\n",
        "        in_features=dim,\n",
        "        hidden_features=hidden_features,\n",
        "        out_features=dim,\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "    \"\"\"\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class VisionTransformers(nn.Module):\n",
        "  \"\"\"Simplified implementation of the Vision Transformer\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  img_size : int\n",
        "    Both height and width of the image (it is a square).\n",
        "  patch_size : int\n",
        "    Both height and width of the patch (it is a square).\n",
        "  in_chans : int\n",
        "    Number of input channels.\n",
        "  n_classes : int\n",
        "    Number of classes.\n",
        "  embed_dim : int\n",
        "    The embedding dimension\n",
        "  depth : int\n",
        "    Number of blocks.\n",
        "  n_heads : int\n",
        "    Number of attention heads.\n",
        "  mlp_ratio : float\n",
        "    Determines the hidden dimension size of the `MLP` module.\n",
        "  qkv_bias : bool\n",
        "    If True then we include bias to the query, key and value projections.\n",
        "  p, attn_p : float\n",
        "    Dropout probability.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  patch_embed : PatchEmbedding\n",
        "    Instance of 'PatchEmbed' layer.\n",
        "  cls_token : nn.Parameter\n",
        "    Learnable parameter that will represent the first token in the sequence.\n",
        "    It has `embed_dim` elements.\n",
        "  pos_embed : nn.Parameter\n",
        "    Positional embedding of the cls token + all the patches.\n",
        "    It has `(n_patches + 1) * embed_dim` elements.\n",
        "  pos_drop : nn.Dropout\n",
        "    Dropout layer.\n",
        "  blocks : nn.ModuleList\n",
        "    List of 'Block' modules.\n",
        "  norm : nn.LayerNorm\n",
        "    Layer normalization\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      img_size=384,\n",
        "      patch_size=16,\n",
        "      in_chans=3,\n",
        "      n_classes=2,\n",
        "      embed_dim=768,\n",
        "      depth=12,\n",
        "      n_heads=12,\n",
        "      mlp_ratio=4.,\n",
        "      qkv_bias=True,\n",
        "      p=0.,\n",
        "      attn_p=0.,\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embed = PatchEmbedding(\n",
        "        img_size=img_size,\n",
        "        patch_size=patch_size,\n",
        "        in_chans=in_chans,\n",
        "        embed_dim=embed_dim,\n",
        "    )\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(\n",
        "        torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
        "    )\n",
        "    self.pos_drop = nn.Dropout(p=p)\n",
        "\n",
        "    self.blocks = nn.ModuleList(\n",
        "        [\n",
        "            Block(\n",
        "                dim=embed_dim,\n",
        "                n_heads=n_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                p=p,\n",
        "                attn_p=attn_p,\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "    self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run the forward\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_chans, img_size, img_size)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    logits : torch.Tensor\n",
        "      Logits over all the classes - '(n_samples, n_classes)'.\n",
        "    \"\"\"\n",
        "    n_samples = x.shape[0]\n",
        "    x = self.patch_embed(x)\n",
        "\n",
        "    cls_token = self.cls_token.expand(\n",
        "        n_samples, -1, -1\n",
        "    )  # (n_samples, 1, embed_dim)\n",
        "    x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
        "    x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
        "    x = self.pos_drop(x)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.norm(x)\n",
        "    cls_token_final = x[:, 0]  # just the CLS token\n",
        "    x = self.head(cls_token_final)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "i-97bxbQB0Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementacja modelu"
      ],
      "metadata": {
        "id": "JpMe_m3fXo6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "o_twxdaKUC2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stałe\n",
        "DATASET_DIR = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Normalizacja i zamiana na skale szarości\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Ładowanie danych\n",
        "data = datasets.ImageFolder(DATASET_DIR, transform=transform)\n",
        "class_names = data.classes\n",
        "\n",
        "# Podział danych\n",
        "train_idx, val_idx = train_test_split(\n",
        "    list(range(len(data))), test_size=0.2, stratify=data.targets, random_state=42\n",
        ")\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "val_data = torch.utils.data.Subset(data, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "4936uToOarJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Wyświetlenie liczby danych w zbiorach treningowych i walidacyjnych\n",
        "print(f\"Liczba próbek w zbiorze treningowym: {len(train_data)}\")\n",
        "print(f\"Liczba próbek w zbiorze walidacyjnym: {len(val_data)}\")\n",
        "\n",
        "# Rozkład klas w zbiorze treningowym\n",
        "train_labels = [data.targets[i] for i in train_idx]\n",
        "train_class_counts = Counter(train_labels)\n",
        "print(\"\\nRozkład klas w zbiorze treningowym:\")\n",
        "for class_name, count in zip(class_names, train_class_counts.values()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "\n",
        "# Rozkład klas w zbiorze walidacyjnym\n",
        "val_labels = [data.targets[i] for i in val_idx]\n",
        "val_class_counts = Counter(val_labels)\n",
        "print(\"\\nRozkład klas w zbiorze walidacyjnym:\")\n",
        "for class_name, count in zip(class_names, val_class_counts.values()):\n",
        "    print(f\"{class_name}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RKXN0A1waza",
        "outputId": "61290ab6-3140-4cf5-d190-1edeabae4e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba próbek w zbiorze treningowym: 7201\n",
            "Liczba próbek w zbiorze walidacyjnym: 1801\n",
            "\n",
            "Rozkład klas w zbiorze treningowym:\n",
            "benign: 4094\n",
            "malignant: 3107\n",
            "\n",
            "Rozkład klas w zbiorze walidacyjnym:\n",
            "benign: 1024\n",
            "malignant: 777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Konfiguracja modelu pod zbiór danych\n",
        "custom_config = {\n",
        "    \"img_size\": IMG_SIZE,\n",
        "    \"patch_size\": 16,\n",
        "    \"in_chans\": 1,\n",
        "    \"n_classes\": len(class_names),\n",
        "    \"embed_dim\": 768,\n",
        "    \"depth\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"qkv_bias\": True,\n",
        "    \"mlp_ratio\": 4.0,\n",
        "    \"p\": 0.1,\n",
        "    \"attn_p\": 0.1,\n",
        "}\n",
        "\n",
        "model = VisionTransformers(**custom_config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Wytyczne treningu\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-7)  # dodanie weight decay\n",
        "EPOCHS = 30\n",
        "\n",
        "# Pętla treningowa\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience=5, delta=0.001):\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        val_loss, val_acc, val_precision, val_recall, val_f1 = evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
        "              f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-score: {val_f1:.4f}\")\n",
        "\n",
        "\n",
        "        # Early Stopping\n",
        "        if val_loss < best_val_loss - delta:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            early_stop_counter = 0\n",
        "            print(f\"New best model found at epoch {epoch + 1}, saving model...\")\n",
        "            torch.save(model.state_dict(), 'best_model.pth')  # Zapisanie najlepszej wersji modelu\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            print(f\"No improvement in validation loss for {early_stop_counter} epoch(s).\")\n",
        "\n",
        "        if early_stop_counter >= patience:\n",
        "            print(f\"Early stopping triggered. Best epoch: {best_epoch + 1} with Val Loss: {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    print(\"Training completed. Best model loaded.\")\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Obliczenie metryk\n",
        "    accuracy = (np.array(all_predictions) == np.array(all_labels)).mean()\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"\\nVal Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n",
        "          f\"Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "    return val_loss, accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "WRAMiKyTa1OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start treningu\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)\n",
        "\n",
        "# Zapisz\n",
        "MODEL_PATH = \"vision_transformer_usg.pth\"\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(f\"Model saved to {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m00EgxzYuMLW",
        "outputId": "b62fa858-0a20-4d8c-e845-9a0f238d17c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Val Loss: 0.7006, Accuracy: 0.4314, Precision: 0.1861, Recall: 0.4314, F1-score: 0.2601\n",
            "Epoch 1/30, Train Loss: 0.7166, Val Loss: 0.7006, Val Acc: 0.4314, Precision: 0.1861, Recall: 0.4314, F1-score: 0.2601\n",
            "New best model found at epoch 1, saving model...\n",
            "\n",
            "Val Loss: 0.6904, Accuracy: 0.5669, Precision: 0.5618, Recall: 0.5669, F1-score: 0.5632\n",
            "Epoch 2/30, Train Loss: 0.6979, Val Loss: 0.6904, Val Acc: 0.5669, Precision: 0.5618, Recall: 0.5669, F1-score: 0.5632\n",
            "New best model found at epoch 2, saving model...\n",
            "\n",
            "Val Loss: 0.6946, Accuracy: 0.4997, Precision: 0.5562, Recall: 0.4997, F1-score: 0.4711\n",
            "Epoch 3/30, Train Loss: 0.6887, Val Loss: 0.6946, Val Acc: 0.4997, Precision: 0.5562, Recall: 0.4997, F1-score: 0.4711\n",
            "No improvement in validation loss for 1 epoch(s).\n",
            "\n",
            "Val Loss: 0.6812, Accuracy: 0.5686, Precision: 0.5774, Recall: 0.5686, F1-score: 0.5706\n",
            "Epoch 4/30, Train Loss: 0.6900, Val Loss: 0.6812, Val Acc: 0.5686, Precision: 0.5774, Recall: 0.5686, F1-score: 0.5706\n",
            "New best model found at epoch 4, saving model...\n",
            "\n",
            "Val Loss: 0.6797, Accuracy: 0.5741, Precision: 0.5768, Recall: 0.5741, F1-score: 0.4478\n",
            "Epoch 5/30, Train Loss: 0.6859, Val Loss: 0.6797, Val Acc: 0.5741, Precision: 0.5768, Recall: 0.5741, F1-score: 0.4478\n",
            "New best model found at epoch 5, saving model...\n",
            "\n",
            "Val Loss: 0.6718, Accuracy: 0.5891, Precision: 0.5889, Recall: 0.5891, F1-score: 0.5154\n",
            "Epoch 6/30, Train Loss: 0.6819, Val Loss: 0.6718, Val Acc: 0.5891, Precision: 0.5889, Recall: 0.5891, F1-score: 0.5154\n",
            "New best model found at epoch 6, saving model...\n",
            "\n",
            "Val Loss: 0.6670, Accuracy: 0.5941, Precision: 0.5837, Recall: 0.5941, F1-score: 0.5709\n",
            "Epoch 7/30, Train Loss: 0.6774, Val Loss: 0.6670, Val Acc: 0.5941, Precision: 0.5837, Recall: 0.5941, F1-score: 0.5709\n",
            "New best model found at epoch 7, saving model...\n",
            "\n",
            "Val Loss: 0.6635, Accuracy: 0.6024, Precision: 0.5949, Recall: 0.6024, F1-score: 0.5929\n",
            "Epoch 8/30, Train Loss: 0.6717, Val Loss: 0.6635, Val Acc: 0.6024, Precision: 0.5949, Recall: 0.6024, F1-score: 0.5929\n",
            "New best model found at epoch 8, saving model...\n",
            "\n",
            "Val Loss: 0.6623, Accuracy: 0.6197, Precision: 0.6146, Recall: 0.6197, F1-score: 0.5976\n",
            "Epoch 9/30, Train Loss: 0.6675, Val Loss: 0.6623, Val Acc: 0.6197, Precision: 0.6146, Recall: 0.6197, F1-score: 0.5976\n",
            "New best model found at epoch 9, saving model...\n",
            "\n",
            "Val Loss: 0.6569, Accuracy: 0.6224, Precision: 0.6162, Recall: 0.6224, F1-score: 0.6098\n",
            "Epoch 10/30, Train Loss: 0.6603, Val Loss: 0.6569, Val Acc: 0.6224, Precision: 0.6162, Recall: 0.6224, F1-score: 0.6098\n",
            "New best model found at epoch 10, saving model...\n",
            "\n",
            "Val Loss: 0.6510, Accuracy: 0.6224, Precision: 0.6163, Recall: 0.6224, F1-score: 0.6085\n",
            "Epoch 11/30, Train Loss: 0.6574, Val Loss: 0.6510, Val Acc: 0.6224, Precision: 0.6163, Recall: 0.6224, F1-score: 0.6085\n",
            "New best model found at epoch 11, saving model...\n",
            "\n",
            "Val Loss: 0.6488, Accuracy: 0.6374, Precision: 0.6495, Recall: 0.6374, F1-score: 0.5997\n",
            "Epoch 12/30, Train Loss: 0.6530, Val Loss: 0.6488, Val Acc: 0.6374, Precision: 0.6495, Recall: 0.6374, F1-score: 0.5997\n",
            "New best model found at epoch 12, saving model...\n",
            "\n",
            "Val Loss: 0.6430, Accuracy: 0.6297, Precision: 0.6312, Recall: 0.6297, F1-score: 0.6000\n",
            "Epoch 13/30, Train Loss: 0.6498, Val Loss: 0.6430, Val Acc: 0.6297, Precision: 0.6312, Recall: 0.6297, F1-score: 0.6000\n",
            "New best model found at epoch 13, saving model...\n",
            "\n",
            "Val Loss: 0.6403, Accuracy: 0.6380, Precision: 0.6484, Recall: 0.6380, F1-score: 0.6021\n",
            "Epoch 14/30, Train Loss: 0.6457, Val Loss: 0.6403, Val Acc: 0.6380, Precision: 0.6484, Recall: 0.6380, F1-score: 0.6021\n",
            "New best model found at epoch 14, saving model...\n",
            "\n",
            "Val Loss: 0.6458, Accuracy: 0.6374, Precision: 0.6667, Recall: 0.6374, F1-score: 0.5864\n",
            "Epoch 15/30, Train Loss: 0.6422, Val Loss: 0.6458, Val Acc: 0.6374, Precision: 0.6667, Recall: 0.6374, F1-score: 0.5864\n",
            "No improvement in validation loss for 1 epoch(s).\n",
            "\n",
            "Val Loss: 0.6301, Accuracy: 0.6408, Precision: 0.6366, Recall: 0.6408, F1-score: 0.6281\n",
            "Epoch 16/30, Train Loss: 0.6316, Val Loss: 0.6301, Val Acc: 0.6408, Precision: 0.6366, Recall: 0.6408, F1-score: 0.6281\n",
            "New best model found at epoch 16, saving model...\n",
            "\n",
            "Val Loss: 0.6287, Accuracy: 0.6441, Precision: 0.6403, Recall: 0.6441, F1-score: 0.6316\n",
            "Epoch 17/30, Train Loss: 0.6270, Val Loss: 0.6287, Val Acc: 0.6441, Precision: 0.6403, Recall: 0.6441, F1-score: 0.6316\n",
            "New best model found at epoch 17, saving model...\n",
            "\n",
            "Val Loss: 0.6249, Accuracy: 0.6513, Precision: 0.6472, Recall: 0.6513, F1-score: 0.6458\n",
            "Epoch 18/30, Train Loss: 0.6204, Val Loss: 0.6249, Val Acc: 0.6513, Precision: 0.6472, Recall: 0.6513, F1-score: 0.6458\n",
            "New best model found at epoch 18, saving model...\n",
            "\n",
            "Val Loss: 0.6166, Accuracy: 0.6485, Precision: 0.6443, Recall: 0.6485, F1-score: 0.6426\n",
            "Epoch 19/30, Train Loss: 0.6152, Val Loss: 0.6166, Val Acc: 0.6485, Precision: 0.6443, Recall: 0.6485, F1-score: 0.6426\n",
            "New best model found at epoch 19, saving model...\n",
            "\n",
            "Val Loss: 0.6128, Accuracy: 0.6607, Precision: 0.6637, Recall: 0.6607, F1-score: 0.6419\n",
            "Epoch 20/30, Train Loss: 0.6043, Val Loss: 0.6128, Val Acc: 0.6607, Precision: 0.6637, Recall: 0.6607, F1-score: 0.6419\n",
            "New best model found at epoch 20, saving model...\n",
            "\n",
            "Val Loss: 0.6186, Accuracy: 0.6541, Precision: 0.6562, Recall: 0.6541, F1-score: 0.6343\n",
            "Epoch 21/30, Train Loss: 0.6015, Val Loss: 0.6186, Val Acc: 0.6541, Precision: 0.6562, Recall: 0.6541, F1-score: 0.6343\n",
            "No improvement in validation loss for 1 epoch(s).\n",
            "\n",
            "Val Loss: 0.6379, Accuracy: 0.6135, Precision: 0.6480, Recall: 0.6135, F1-score: 0.6109\n",
            "Epoch 22/30, Train Loss: 0.5936, Val Loss: 0.6379, Val Acc: 0.6135, Precision: 0.6480, Recall: 0.6135, F1-score: 0.6109\n",
            "No improvement in validation loss for 2 epoch(s).\n",
            "\n",
            "Val Loss: 0.5998, Accuracy: 0.6563, Precision: 0.6524, Recall: 0.6563, F1-score: 0.6502\n",
            "Epoch 23/30, Train Loss: 0.5801, Val Loss: 0.5998, Val Acc: 0.6563, Precision: 0.6524, Recall: 0.6563, F1-score: 0.6502\n",
            "New best model found at epoch 23, saving model...\n",
            "\n",
            "Val Loss: 0.6128, Accuracy: 0.6613, Precision: 0.6687, Recall: 0.6613, F1-score: 0.6380\n",
            "Epoch 24/30, Train Loss: 0.5871, Val Loss: 0.6128, Val Acc: 0.6613, Precision: 0.6687, Recall: 0.6613, F1-score: 0.6380\n",
            "No improvement in validation loss for 1 epoch(s).\n",
            "\n",
            "Val Loss: 0.6140, Accuracy: 0.6602, Precision: 0.6570, Recall: 0.6602, F1-score: 0.6571\n",
            "Epoch 25/30, Train Loss: 0.5638, Val Loss: 0.6140, Val Acc: 0.6602, Precision: 0.6570, Recall: 0.6602, F1-score: 0.6571\n",
            "No improvement in validation loss for 2 epoch(s).\n",
            "\n",
            "Val Loss: 0.6195, Accuracy: 0.6646, Precision: 0.6612, Recall: 0.6646, F1-score: 0.6603\n",
            "Epoch 26/30, Train Loss: 0.5521, Val Loss: 0.6195, Val Acc: 0.6646, Precision: 0.6612, Recall: 0.6646, F1-score: 0.6603\n",
            "No improvement in validation loss for 3 epoch(s).\n",
            "\n",
            "Val Loss: 0.6123, Accuracy: 0.6663, Precision: 0.6681, Recall: 0.6663, F1-score: 0.6670\n",
            "Epoch 27/30, Train Loss: 0.5497, Val Loss: 0.6123, Val Acc: 0.6663, Precision: 0.6681, Recall: 0.6663, F1-score: 0.6670\n",
            "No improvement in validation loss for 4 epoch(s).\n",
            "\n",
            "Val Loss: 0.6136, Accuracy: 0.6596, Precision: 0.6565, Recall: 0.6596, F1-score: 0.6503\n",
            "Epoch 28/30, Train Loss: 0.5386, Val Loss: 0.6136, Val Acc: 0.6596, Precision: 0.6565, Recall: 0.6596, F1-score: 0.6503\n",
            "No improvement in validation loss for 5 epoch(s).\n",
            "Early stopping triggered. Best epoch: 23 with Val Loss: 0.5998\n",
            "Training completed. Best model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-741f6efe3c11>:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to vision_transformer_usg.pth\n"
          ]
        }
      ]
    }
  ]
}