{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "1zBD2ipDlKfx2YCpEjEBgU7W806NeIPhJ",
      "authorship_tag": "ABX9TyOH6u+YhskvmZM8aRosFNL7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wujciak/thesis_colab/blob/main/transformers_thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architektura Vision Transformer zaproponowana przez Google\n"
      ],
      "metadata": {
        "id": "5Kfe3cnsXtYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "DNFe5qiG7hZJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"Split image into patches and then embed them.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  img_size : int\n",
        "    Size of the image (it is a square).\n",
        "  patch_size : int\n",
        "    Size of the patch (it is a square).\n",
        "  in_chans : int\n",
        "    Number of input channels.\n",
        "  embed_dim : int\n",
        "    The embedding dimension.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  n_patches : int\n",
        "    Number of patches inside of our image.\n",
        "  proj : nn.Conv2d\n",
        "    Convolutional layer that does both the splitting into patches\n",
        "    and their embedding.\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
        "    super().__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "    self.proj = nn.Conv2d(\n",
        "        in_chans,\n",
        "        embed_dim,\n",
        "        kernel_size=patch_size,\n",
        "        stride=patch_size,\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_chans, img_size, img_size)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches, embed_dim)'.\n",
        "    \"\"\"\n",
        "    x = self.proj(\n",
        "        x\n",
        "    )  # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
        "    x = x.flatten(2)  # (n_samples, embed_dim, n_patches)\n",
        "    x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  \"\"\"Multi-head attention mechanism.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dim : int\n",
        "    The input and out dimension of per token features.\n",
        "  n_heads : int\n",
        "    Number of attention heads.\n",
        "  qkv_bias : bool\n",
        "    If True then we include bias to the query, key and value projections.\n",
        "  attn_p : float\n",
        "    Dropout probability applied to the query, key and value tensors.\n",
        "  proj_p : float\n",
        "    Dropout probability applied to the output tensor.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  scale : float\n",
        "    Normalizing consant for the dot product.\n",
        "  qkv : nn.Linear\n",
        "    Linear projection for the query, key and value.\n",
        "  proj : nn.Linear\n",
        "    Linear mapping that takes in the concatenated output of all attention\n",
        "    heads and maps it into a new space.\n",
        "  attn_drop, proj_drop : nn.Dropout\n",
        "    Dropout layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.dim = dim\n",
        "    self.head_dim = dim // n_heads\n",
        "    self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "    \"\"\"\n",
        "    n_samples, n_tokens, dim = x.shape\n",
        "\n",
        "    if dim != self.dim:\n",
        "      raise ValueError\n",
        "\n",
        "    qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
        "    qkv = qkv.reshape(\n",
        "        n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
        "    )  # (n_samples, n_patches + 1, 3, n_heads, head_dim)\n",
        "    qkv = qkv.permute(\n",
        "        2, 0, 3, 1, 4\n",
        "    )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
        "\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "    k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
        "    dp = (\n",
        "        q @ k_t\n",
        "    ) * self.scale  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "    attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    weighted_avg = attn @ v  # (n_samples, n_heads, n_patches + 1, head_dim)\n",
        "    weighted_avg = weighted_avg.transpose(\n",
        "        1, 2\n",
        "    )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
        "    weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "    x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
        "    x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  \"\"\"Multi-layer perceptron.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  in_features : int\n",
        "    Number of input features\n",
        "  hidden_features : int\n",
        "    Number of nodes in the hidden layer\n",
        "  out_features : int\n",
        "    Number of output features\n",
        "  p : float\n",
        "    Dropout probability\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  fc : nn.Linear\n",
        "    The first linear layer\n",
        "  act : nn.GELU\n",
        "    GELU activation function\n",
        "  fc2 : nn.Linear\n",
        "    The second linear layer\n",
        "  drop : nn.Dropout\n",
        "    Dropout layer\n",
        "  \"\"\"\n",
        "  def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = nn.GELU()\n",
        "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "    self.drop = nn.Dropout(p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_features)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, out_features)'.\n",
        "    \"\"\"\n",
        "    x = self.fc1(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer block.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dim : int\n",
        "    Embedding dimension\n",
        "  n_heads : int\n",
        "    Number of attention heads\n",
        "  mlp_ratio : float\n",
        "    Determines the hidden dimension size of the `MLP` module with respect\n",
        "    to `dim`\n",
        "  qkv_bias : bool\n",
        "    If True we include bias to the query, key and value projections\n",
        "  p, attn_p : float\n",
        "    Dropout probability\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  norm1, norm2 : nn.LayerNorm\n",
        "    Layer normalization\n",
        "  attn : Attention\n",
        "    Attention module\n",
        "  mlp : MLP\n",
        "    MLP module.\n",
        "  \"\"\"\n",
        "  def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    self.attn = Attention(\n",
        "        dim,\n",
        "        n_heads=n_heads,\n",
        "        qkv_bias=qkv_bias,\n",
        "        attn_p=attn_p,\n",
        "        proj_p=p\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    hidden_features = int(dim * mlp_ratio)\n",
        "    self.mlp = MLP(\n",
        "        in_features=dim,\n",
        "        hidden_features=hidden_features,\n",
        "        out_features=dim,\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "    \"\"\"\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class VisionTransformers(nn.Module):\n",
        "  \"\"\"Simplified implementation of the Vision Transformer\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  img_size : int\n",
        "    Both height and width of the image (it is a square).\n",
        "  patch_size : int\n",
        "    Both height and width of the patch (it is a square).\n",
        "  in_chans : int\n",
        "    Number of input channels.\n",
        "  n_classes : int\n",
        "    Number of classes.\n",
        "  embed_dim : int\n",
        "    The embedding dimension\n",
        "  depth : int\n",
        "    Number of blocks.\n",
        "  n_heads : int\n",
        "    Number of attention heads.\n",
        "  mlp_ratio : float\n",
        "    Determines the hidden dimension size of the `MLP` module.\n",
        "  qkv_bias : bool\n",
        "    If True then we include bias to the query, key and value projections.\n",
        "  p, attn_p : float\n",
        "    Dropout probability.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  patch_embed : PatchEmbedding\n",
        "    Instance of 'PatchEmbed' layer.\n",
        "  cls_token : nn.Parameter\n",
        "    Learnable parameter that will represent the first token in the sequence.\n",
        "    It has `embed_dim` elements.\n",
        "  pos_embed : nn.Parameter\n",
        "    Positional embedding of the cls token + all the patches.\n",
        "    It has `(n_patches + 1) * embed_dim` elements.\n",
        "  pos_drop : nn.Dropout\n",
        "    Dropout layer.\n",
        "  blocks : nn.ModuleList\n",
        "    List of 'Block' modules.\n",
        "  norm : nn.LayerNorm\n",
        "    Layer normalization\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      img_size=384,\n",
        "      patch_size=16,\n",
        "      in_chans=3,\n",
        "      n_classes=2,\n",
        "      embed_dim=768,\n",
        "      depth=12,\n",
        "      n_heads=12,\n",
        "      mlp_ratio=4.,\n",
        "      qkv_bias=True,\n",
        "      p=0.,\n",
        "      attn_p=0.,\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embed = PatchEmbedding(\n",
        "        img_size=img_size,\n",
        "        patch_size=patch_size,\n",
        "        in_chans=in_chans,\n",
        "        embed_dim=embed_dim,\n",
        "    )\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(\n",
        "        torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
        "    )\n",
        "    self.pos_drop = nn.Dropout(p=p)\n",
        "\n",
        "    self.blocks = nn.ModuleList(\n",
        "        [\n",
        "            Block(\n",
        "                dim=embed_dim,\n",
        "                n_heads=n_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                p=p,\n",
        "                attn_p=attn_p,\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "    self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run the forward\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_chans, img_size, img_size)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    logits : torch.Tensor\n",
        "      Logits over all the classes - '(n_samples, n_classes)'.\n",
        "    \"\"\"\n",
        "    n_samples = x.shape[0]\n",
        "    x = self.patch_embed(x)\n",
        "\n",
        "    cls_token = self.cls_token.expand(\n",
        "        n_samples, -1, -1\n",
        "    )  # (n_samples, 1, embed_dim)\n",
        "    x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
        "    x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
        "    x = self.pos_drop(x)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.norm(x)\n",
        "    cls_token_final = x[:, 0]  # just the CLS token\n",
        "    x = self.head(cls_token_final)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "i-97bxbQB0Dg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementacja modelu"
      ],
      "metadata": {
        "id": "JpMe_m3fXo6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "o_twxdaKUC2Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stałe\n",
        "DATASET_DIR = \"/content/drive/MyDrive/Colab Notebooks/alg_and_batch1\"\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Normalizacja i zamiana na skale szarości\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Ładowanie danych\n",
        "data = datasets.ImageFolder(DATASET_DIR, transform=transform)\n",
        "class_names = data.classes\n",
        "\n",
        "# Podział danych\n",
        "train_idx, val_idx = train_test_split(\n",
        "    list(range(len(data))), test_size=0.2, stratify=data.targets, random_state=42\n",
        ")\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "val_data = torch.utils.data.Subset(data, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "4936uToOarJB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Wyświetlenie liczby danych w zbiorach treningowych i walidacyjnych\n",
        "print(f\"Liczba próbek w zbiorze treningowym: {len(train_data)}\")\n",
        "print(f\"Liczba próbek w zbiorze walidacyjnym: {len(val_data)}\")\n",
        "\n",
        "# Rozkład klas w zbiorze treningowym\n",
        "train_labels = [data.targets[i] for i in train_idx]\n",
        "train_class_counts = Counter(train_labels)\n",
        "print(\"\\nRozkład klas w zbiorze treningowym:\")\n",
        "for class_name, count in zip(class_names, train_class_counts.values()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "\n",
        "# Rozkład klas w zbiorze walidacyjnym\n",
        "val_labels = [data.targets[i] for i in val_idx]\n",
        "val_class_counts = Counter(val_labels)\n",
        "print(\"\\nRozkład klas w zbiorze walidacyjnym:\")\n",
        "for class_name, count in zip(class_names, val_class_counts.values()):\n",
        "    print(f\"{class_name}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RKXN0A1waza",
        "outputId": "942aa9f5-e99c-4d8a-be7c-c80ec7b6d44c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba próbek w zbiorze treningowym: 4995\n",
            "Liczba próbek w zbiorze walidacyjnym: 1249\n",
            "\n",
            "Rozkład klas w zbiorze treningowym:\n",
            "benign: 2299\n",
            "malignant: 2696\n",
            "\n",
            "Rozkład klas w zbiorze walidacyjnym:\n",
            "benign: 575\n",
            "malignant: 674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfiguracja modelu pod zbiór danych\n",
        "custom_config = {\n",
        "    \"img_size\": IMG_SIZE,\n",
        "    \"patch_size\": 16,\n",
        "    \"in_chans\": 1,\n",
        "    \"n_classes\": len(class_names),\n",
        "    \"embed_dim\": 768,\n",
        "    \"depth\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"qkv_bias\": True,\n",
        "    \"mlp_ratio\": 4.0,\n",
        "    \"p\": 0.1,\n",
        "    \"attn_p\": 0.1,\n",
        "}\n",
        "\n",
        "model = VisionTransformers(**custom_config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Wytyczne treningu\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=3e-6)  # dodanie weight decay\n",
        "EPOCHS = 20\n",
        "\n",
        "# Pętla treningowa\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "WRAMiKyTa1OQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)\n",
        "\n",
        "# Save trained model\n",
        "MODEL_PATH = \"vision_transformer_usg.pth\"\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(f\"Model saved to {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m00EgxzYuMLW",
        "outputId": "0abfce83-2f7d-4027-bd9e-f4015973d0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.7090, Val Loss: 0.7298, Val Acc: 0.4604\n",
            "Epoch 2/20, Train Loss: 0.6968, Val Loss: 0.7561, Val Acc: 0.4604\n",
            "Epoch 3/20, Train Loss: 0.6862, Val Loss: 0.6752, Val Acc: 0.5893\n",
            "Epoch 4/20, Train Loss: 0.6691, Val Loss: 0.6834, Val Acc: 0.5805\n",
            "Epoch 5/20, Train Loss: 0.6626, Val Loss: 0.7255, Val Acc: 0.5612\n",
            "Epoch 6/20, Train Loss: 0.6531, Val Loss: 0.6787, Val Acc: 0.5813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_21GzknOJy3F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jedno ze szkoleń:"
      ],
      "metadata": {
        "id": "zQSpwrCta--o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Zrzut ekranu 2024-12-19 203603.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "SKcXW6_cam8n"
      }
    }
  ]
}