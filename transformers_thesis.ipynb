{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "1zBD2ipDlKfx2YCpEjEBgU7W806NeIPhJ",
      "authorship_tag": "ABX9TyPtDO7MBkkQnBvjoPl188LI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wujciak/thesis_colab/blob/main/transformers_thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architektura Vision Transformer zaproponowana przez Google\n"
      ],
      "metadata": {
        "id": "5Kfe3cnsXtYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "DNFe5qiG7hZJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"Split image into patches and then embed them.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  img_size : int\n",
        "    Size of the image (it is a square).\n",
        "  patch_size : int\n",
        "    Size of the patch (it is a square).\n",
        "  in_chans : int\n",
        "    Number of input channels.\n",
        "  embed_dim : int\n",
        "    The embedding dimension.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  n_patches : int\n",
        "    Number of patches inside of our image.\n",
        "  proj : nn.Conv2d\n",
        "    Convolutional layer that does both the splitting into patches\n",
        "    and their embedding.\n",
        "  \"\"\"\n",
        "  def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
        "    super().__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "    self.proj = nn.Conv2d(\n",
        "        in_chans,\n",
        "        embed_dim,\n",
        "        kernel_size=patch_size,\n",
        "        stride=patch_size,\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_chans, img_size, img_size)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches, embed_dim)'.\n",
        "    \"\"\"\n",
        "    x = self.proj(\n",
        "        x\n",
        "    )  # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
        "    x = x.flatten(2)  # (n_samples, embed_dim, n_patches)\n",
        "    x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  \"\"\"Multi-head attention mechanism.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dim : int\n",
        "    The input and out dimension of per token features.\n",
        "  n_heads : int\n",
        "    Number of attention heads.\n",
        "  qkv_bias : bool\n",
        "    If True then we include bias to the query, key and value projections.\n",
        "  attn_p : float\n",
        "    Dropout probability applied to the query, key and value tensors.\n",
        "  proj_p : float\n",
        "    Dropout probability applied to the output tensor.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  scale : float\n",
        "    Normalizing consant for the dot product.\n",
        "  qkv : nn.Linear\n",
        "    Linear projection for the query, key and value.\n",
        "  proj : nn.Linear\n",
        "    Linear mapping that takes in the concatenated output of all attention\n",
        "    heads and maps it into a new space.\n",
        "  attn_drop, proj_drop : nn.Dropout\n",
        "    Dropout layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.dim = dim\n",
        "    self.head_dim = dim // n_heads\n",
        "    self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "    \"\"\"\n",
        "    n_samples, n_tokens, dim = x.shape\n",
        "\n",
        "    if dim != self.dim:\n",
        "      raise ValueError\n",
        "\n",
        "    qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
        "    qkv = qkv.reshape(\n",
        "        n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
        "    )  # (n_samples, n_patches + 1, 3, n_heads, head_dim)\n",
        "    qkv = qkv.permute(\n",
        "        2, 0, 3, 1, 4\n",
        "    )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
        "\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "    k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
        "    dp = (\n",
        "        q @ k_t\n",
        "    ) * self.scale  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "    attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    weighted_avg = attn @ v  # (n_samples, n_heads, n_patches + 1, head_dim)\n",
        "    weighted_avg = weighted_avg.transpose(\n",
        "        1, 2\n",
        "    )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
        "    weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "    x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
        "    x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  \"\"\"Multi-layer perceptron.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  in_features : int\n",
        "    Number of input features\n",
        "  hidden_features : int\n",
        "    Number of nodes in the hidden layer\n",
        "  out_features : int\n",
        "    Number of output features\n",
        "  p : float\n",
        "    Dropout probability\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  fc : nn.Linear\n",
        "    The first linear layer\n",
        "  act : nn.GELU\n",
        "    GELU activation function\n",
        "  fc2 : nn.Linear\n",
        "    The second linear layer\n",
        "  drop : nn.Dropout\n",
        "    Dropout layer\n",
        "  \"\"\"\n",
        "  def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "    self.act = nn.GELU()\n",
        "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "    self.drop = nn.Dropout(p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_features)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, out_features)'.\n",
        "    \"\"\"\n",
        "    x = self.fc1(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "    x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer block.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dim : int\n",
        "    Embedding dimension\n",
        "  n_heads : int\n",
        "    Number of attention heads\n",
        "  mlp_ratio : float\n",
        "    Determines the hidden dimension size of the `MLP` module with respect\n",
        "    to `dim`\n",
        "  qkv_bias : bool\n",
        "    If True we include bias to the query, key and value projections\n",
        "  p, attn_p : float\n",
        "    Dropout probability\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  norm1, norm2 : nn.LayerNorm\n",
        "    Layer normalization\n",
        "  attn : Attention\n",
        "    Attention module\n",
        "  mlp : MLP\n",
        "    MLP module.\n",
        "  \"\"\"\n",
        "  def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    self.attn = Attention(\n",
        "        dim,\n",
        "        n_heads=n_heads,\n",
        "        qkv_bias=qkv_bias,\n",
        "        attn_p=attn_p,\n",
        "        proj_p=p\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "    hidden_features = int(dim * mlp_ratio)\n",
        "    self.mlp = MLP(\n",
        "        in_features=dim,\n",
        "        hidden_features=hidden_features,\n",
        "        out_features=dim,\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run forward pass.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      Shape '(n_samples, n_patches + 1, dim)'.\n",
        "    \"\"\"\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class VisionTransformers(nn.Module):\n",
        "  \"\"\"Simplified implementation of the Vision Transformer\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  img_size : int\n",
        "    Both height and width of the image (it is a square).\n",
        "  patch_size : int\n",
        "    Both height and width of the patch (it is a square).\n",
        "  in_chans : int\n",
        "    Number of input channels.\n",
        "  n_classes : int\n",
        "    Number of classes.\n",
        "  embed_dim : int\n",
        "    The embedding dimension\n",
        "  depth : int\n",
        "    Number of blocks.\n",
        "  n_heads : int\n",
        "    Number of attention heads.\n",
        "  mlp_ratio : float\n",
        "    Determines the hidden dimension size of the `MLP` module.\n",
        "  qkv_bias : bool\n",
        "    If True then we include bias to the query, key and value projections.\n",
        "  p, attn_p : float\n",
        "    Dropout probability.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  patch_embed : PatchEmbedding\n",
        "    Instance of 'PatchEmbed' layer.\n",
        "  cls_token : nn.Parameter\n",
        "    Learnable parameter that will represent the first token in the sequence.\n",
        "    It has `embed_dim` elements.\n",
        "  pos_embed : nn.Parameter\n",
        "    Positional embedding of the cls token + all the patches.\n",
        "    It has `(n_patches + 1) * embed_dim` elements.\n",
        "  pos_drop : nn.Dropout\n",
        "    Dropout layer.\n",
        "  blocks : nn.ModuleList\n",
        "    List of 'Block' modules.\n",
        "  norm : nn.LayerNorm\n",
        "    Layer normalization\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      img_size=384,\n",
        "      patch_size=16,\n",
        "      in_chans=3,\n",
        "      n_classes=2,\n",
        "      embed_dim=768,\n",
        "      depth=12,\n",
        "      n_heads=12,\n",
        "      mlp_ratio=4.,\n",
        "      qkv_bias=True,\n",
        "      p=0.,\n",
        "      attn_p=0.,\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_embed = PatchEmbedding(\n",
        "        img_size=img_size,\n",
        "        patch_size=patch_size,\n",
        "        in_chans=in_chans,\n",
        "        embed_dim=embed_dim,\n",
        "    )\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(\n",
        "        torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
        "    )\n",
        "    self.pos_drop = nn.Dropout(p=p)\n",
        "\n",
        "    self.blocks = nn.ModuleList(\n",
        "        [\n",
        "            Block(\n",
        "                dim=embed_dim,\n",
        "                n_heads=n_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                p=p,\n",
        "                attn_p=attn_p,\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "    self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Run the forward\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Shape '(n_samples, in_chans, img_size, img_size)'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    logits : torch.Tensor\n",
        "      Logits over all the classes - '(n_samples, n_classes)'.\n",
        "    \"\"\"\n",
        "    n_samples = x.shape[0]\n",
        "    x = self.patch_embed(x)\n",
        "\n",
        "    cls_token = self.cls_token.expand(\n",
        "        n_samples, -1, -1\n",
        "    )  # (n_samples, 1, embed_dim)\n",
        "    x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
        "    x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
        "    x = self.pos_drop(x)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.norm(x)\n",
        "    cls_token_final = x[:, 0]  # just the CLS token\n",
        "    x = self.head(cls_token_final)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "i-97bxbQB0Dg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementacja modelu"
      ],
      "metadata": {
        "id": "JpMe_m3fXo6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "o_twxdaKUC2Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stałe\n",
        "DATASET_DIR = \"/content/drive/MyDrive/Colab Notebooks/alg_and_batch1\"\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Normalizacja i zamiana na skale szarości\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Ładowanie danych\n",
        "data = datasets.ImageFolder(DATASET_DIR, transform=transform)\n",
        "class_names = data.classes\n",
        "\n",
        "# Podział danych\n",
        "train_idx, val_idx = train_test_split(\n",
        "    list(range(len(data))), test_size=0.2, stratify=data.targets, random_state=42\n",
        ")\n",
        "train_data = torch.utils.data.Subset(data, train_idx)\n",
        "val_data = torch.utils.data.Subset(data, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "4936uToOarJB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Wyświetlenie liczby danych w zbiorach treningowych i walidacyjnych\n",
        "print(f\"Liczba próbek w zbiorze treningowym: {len(train_data)}\")\n",
        "print(f\"Liczba próbek w zbiorze walidacyjnym: {len(val_data)}\")\n",
        "\n",
        "# Rozkład klas w zbiorze treningowym\n",
        "train_labels = [data.targets[i] for i in train_idx]\n",
        "train_class_counts = Counter(train_labels)\n",
        "print(\"\\nRozkład klas w zbiorze treningowym:\")\n",
        "for class_name, count in zip(class_names, train_class_counts.values()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "\n",
        "# Rozkład klas w zbiorze walidacyjnym\n",
        "val_labels = [data.targets[i] for i in val_idx]\n",
        "val_class_counts = Counter(val_labels)\n",
        "print(\"\\nRozkład klas w zbiorze walidacyjnym:\")\n",
        "for class_name, count in zip(class_names, val_class_counts.values()):\n",
        "    print(f\"{class_name}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RKXN0A1waza",
        "outputId": "942aa9f5-e99c-4d8a-be7c-c80ec7b6d44c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba próbek w zbiorze treningowym: 4995\n",
            "Liczba próbek w zbiorze walidacyjnym: 1249\n",
            "\n",
            "Rozkład klas w zbiorze treningowym:\n",
            "benign: 2299\n",
            "malignant: 2696\n",
            "\n",
            "Rozkład klas w zbiorze walidacyjnym:\n",
            "benign: 575\n",
            "malignant: 674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfiguracja modelu pod zbiór danych\n",
        "custom_config = {\n",
        "    \"img_size\": IMG_SIZE,\n",
        "    \"patch_size\": 16,\n",
        "    \"in_chans\": 1,\n",
        "    \"n_classes\": len(class_names),\n",
        "    \"embed_dim\": 768,\n",
        "    \"depth\": 12,\n",
        "    \"n_heads\": 12,\n",
        "    \"qkv_bias\": True,\n",
        "    \"mlp_ratio\": 4.0,\n",
        "    \"p\": 0.1,\n",
        "    \"attn_p\": 0.1,\n",
        "}\n",
        "\n",
        "model = VisionTransformers(**custom_config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Wytyczne treningu\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=3e-6)  # dodanie weight decay\n",
        "EPOCHS = 20\n",
        "\n",
        "# Pętla treningowa\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "WRAMiKyTa1OQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)\n",
        "\n",
        "# Save trained model\n",
        "MODEL_PATH = \"vision_transformer_usg.pth\"\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(f\"Model saved to {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m00EgxzYuMLW",
        "outputId": "12952199-0251-4601-90b1-a04f9fc280dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.7090, Val Loss: 0.7298, Val Acc: 0.4604\n",
            "Epoch 2/20, Train Loss: 0.6968, Val Loss: 0.7561, Val Acc: 0.4604\n",
            "Epoch 3/20, Train Loss: 0.6862, Val Loss: 0.6752, Val Acc: 0.5893\n",
            "Epoch 4/20, Train Loss: 0.6691, Val Loss: 0.6834, Val Acc: 0.5805\n",
            "Epoch 5/20, Train Loss: 0.6626, Val Loss: 0.7255, Val Acc: 0.5612\n",
            "Epoch 6/20, Train Loss: 0.6531, Val Loss: 0.6787, Val Acc: 0.5813\n",
            "Epoch 7/20, Train Loss: 0.6339, Val Loss: 0.7083, Val Acc: 0.5925\n",
            "Epoch 8/20, Train Loss: 0.6161, Val Loss: 0.6847, Val Acc: 0.6085\n",
            "Epoch 9/20, Train Loss: 0.6084, Val Loss: 0.6719, Val Acc: 0.6141\n",
            "Epoch 10/20, Train Loss: 0.5921, Val Loss: 0.7010, Val Acc: 0.6109\n",
            "Epoch 11/20, Train Loss: 0.5824, Val Loss: 0.6893, Val Acc: 0.6101\n",
            "Epoch 12/20, Train Loss: 0.5654, Val Loss: 0.6594, Val Acc: 0.6165\n",
            "Epoch 13/20, Train Loss: 0.5524, Val Loss: 0.6909, Val Acc: 0.6125\n",
            "Epoch 14/20, Train Loss: 0.5331, Val Loss: 0.7402, Val Acc: 0.6149\n",
            "Epoch 15/20, Train Loss: 0.5202, Val Loss: 0.7171, Val Acc: 0.5981\n",
            "Epoch 16/20, Train Loss: 0.4944, Val Loss: 0.7832, Val Acc: 0.6237\n",
            "Epoch 17/20, Train Loss: 0.4890, Val Loss: 0.7756, Val Acc: 0.6133\n",
            "Epoch 18/20, Train Loss: 0.4616, Val Loss: 0.7384, Val Acc: 0.6277\n",
            "Epoch 19/20, Train Loss: 0.4390, Val Loss: 0.8425, Val Acc: 0.6397\n",
            "Epoch 20/20, Train Loss: 0.4321, Val Loss: 0.7846, Val Acc: 0.6293\n",
            "Model saved to vision_transformer_usg.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kod ze statystykami"
      ],
      "metadata": {
        "id": "o6BFFRIzYpv9"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}